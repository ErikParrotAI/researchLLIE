# -*- coding: utf-8 -*-
"""
Retinexformer¬†Training¬†(PyTorch)
===============================

**–ü—Ä–æ–±–ª–µ–º–∞ –ø–∞–º º—è—Ç—ñ –≤–∏—Ä—ñ—à–µ–Ω–∞¬†‚û°Ô∏è¬†—Ç–µ–ø–µ—Ä –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø–∞—Ç—á‚Äë–µ–º–±–µ–¥–∏–Ω–≥**

–£ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –≤–µ—Ä—Å—ñ—ó –ø–æ–≤–Ω–∏–π self‚Äëattention –ø—Ä–∞—Ü—é–≤–∞–≤ –Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ `H√óW`‚Äë—Ç–æ–∫–µ–Ω—ñ–≤ (–¥–ª—è –∫—Ä–æ–ø–∞
256√ó256 —Ü–µ¬†65‚ÄØ536 —Ç–æ–∫–µ–Ω—ñ–≤ ‚Üí –º–∞—Ç—Ä–∏—Ü—è —É–≤–∞–≥–∏ ~‚ÄØ1‚ÄØ–¢–ë). –ù–æ–≤–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤–∏–∫–æ–Ω—É—î self‚Äëattention
–Ω–∞ —Ä—ñ–≤–Ω—ñ **–ø–∞—Ç—á—ñ–≤** (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º¬†8‚ÄØ√ó‚ÄØ8), —Ç–æ–º—É –¥–æ–≤–∂–∏–Ω–∞ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ ‚Üí `(H/8)√ó(W/8)`
(–¥–ª—è 256‚ÄØ√ó‚ÄØ256 —Ü–µ –ª–∏—à–µ¬†1‚ÄØ024 —Ç–æ–∫–µ–Ω–∏) ‚Äî –ø–∞–º º—è—Ç—ñ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –Ω–∞–≤—ñ—Ç—å –Ω–∞ CPU.

–ì–æ–ª–æ–≤–Ω—ñ –∑–º—ñ–Ω–∏
-------------
* **Retinexformer**:
  * `patch_size` (def¬†=¬†8). –ü–∞—Ç—á‚Äë–µ–º–±–µ–¥–∏–Ω–≥¬†‚Äî `Conv2d(stride=patch)`.
  * –ü—ñ—Å–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞¬†‚Äî reshape ‚Üí upsample `F.interpolate(scale_factor=patch)`.
* **Dataset**: –∫—Ä–æ–ø —Ä–æ–∑–º—ñ—Ä–æ–º, –∫—Ä–∞—Ç–Ω–∏–º `patch_size` (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º 256, —â–æ –¥—ñ–ª–∏—Ç—å—Å—è –Ω–∞¬†8).
* **CLI**‚ÄØ‚Äì¬†–¥–æ–¥–∞–Ω–æ `--patch`.

–ó–∞–ø—É—Å–∫ –ø—Ä–∏–∫–ª–∞–¥–æ–º:
```bash
python retinexformer_training.py \
  --dataset_root "D:/Python/researchLLIE/data/lol_dataset/our485" \
  --epochs 100 --batch_size 8 --patch 8
```

---
"""

from __future__ import annotations

import argparse
import os
from pathlib import Path
from typing import List, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from torchvision.transforms import functional as TF
from PIL import Image

try:
    from torchmetrics.functional import structural_similarity_index_measure as ssim_fn
except ImportError:
    ssim_fn = None  # —è–∫—â–æ torchmetrics –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ ‚Äì –±—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –ª–∏—à–µ L1

# ---------------------------
# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
# ---------------------------
DEF_DS_ROOT = "../data/lol_dataset/our485"
DEF_EPOCHS = 100
DEF_BS = 8
DEF_CROP = 256
DEF_LR = 1e-4
DEF_PATCH = 8  # –Ω–æ–≤–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä

# ---------------------------
# –î–∞—Ç–∞—Å–µ—Ç LOL
# ---------------------------
class LOLPairDataset(Dataset):
    """Dataset LOL —ñ–∑ –∫–ª–∞—Å–∏—á–Ω–æ—é —Å—Ç—Ä—É–∫—Ç—É—Ä–æ—é low/ —Ç–∞ high/."""

    def __init__(self, root: str | Path, crop_size: int = 256, patch_size: int = 8):
        self.root = Path(root)
        self.crop_size = crop_size
        self.patch_size = patch_size
        if self.crop_size % self.patch_size != 0:
            raise ValueError(
                f"crop_size ({self.crop_size}) –º–∞—î –¥—ñ–ª–∏—Ç–∏—Å—è –Ω–∞ patch_size ({self.patch_size})"
            )

        if not self.root.exists():
            raise FileNotFoundError(
                f"–ö–∞—Ç–∞–ª–æ–≥ {self.root} –Ω–µ —ñ—Å–Ω—É—î¬†‚Äì –ø–µ—Ä–µ–≤—ñ—Ä—Ç–µ —à–ª—è—Ö –¥–æ –¥–∞—Ç–∞—Å–µ—Ç—É."
            )

        low_dir = self.root / "low"
        high_dir = self.root / "high"
        if not low_dir.exists() or not high_dir.exists():
            raise RuntimeError(
                "–û—á—ñ–∫—É—î—Ç—å—Å—è, —â–æ –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ root –±—É–¥—É—Ç—å –ø—ñ–¥–ø–∞–ø–∫–∏ 'low' —Ç–∞ 'high'. "
                "–ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç—É."
            )

        # –ü–æ—à—É–∫ —Å–ø—ñ–ª—å–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤ –∑–∞ —ñ–º–µ–Ω–µ–º —Ç–∞ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è–º
        self.pairs: List[Tuple[Path, Path]] = []
        for low_path in low_dir.rglob("*.*"):
            if low_path.suffix.lower() not in {".png", ".jpg", ".jpeg"}:
                continue
            rel_name = low_path.relative_to(low_dir)
            high_path = high_dir / rel_name
            if high_path.exists():
                self.pairs.append((low_path, high_path))

        if len(self.pairs) == 0:
            raise RuntimeError(
                f"–£ –∫–∞—Ç–∞–ª–æ–∑—ñ {self.root} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∑–±—ñ–≥—ñ–≤ –º—ñ–∂ 'low' —Ç–∞ 'high'. "
                "–ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è, —â–æ —ñ–º–µ–Ω–∞ —Ñ–∞–π–ª—ñ–≤ —É –æ–±–æ—Ö –ø–∞–ø–∫–∞—Ö –∑–±—ñ–≥–∞—é—Ç—å—Å—è."
            )
        print(f"[Dataset] –ó–Ω–∞–π–¥–µ–Ω–æ {len(self.pairs)} –ø–∞—Ä low/high —É {self.root}")

        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó
        self.to_tensor = transforms.ToTensor()

    # ----------------------------------------
    def __len__(self):  # type: ignore[override]
        return len(self.pairs)

    def random_crop(self, low: Image.Image, high: Image.Image):
        """RandomCrop, –±–µ–∑–ø–µ—á–Ω–∏–π, –≥–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–æ –∫—Ä–∞—Ç–Ω–∏–π patch_size."""
        if low.height < self.crop_size or low.width < self.crop_size:
            # fallback¬†‚Äì —Ü–µ–Ω—Ç—Ä‚Äë–∫—Ä–æ–ø –º—ñ–Ω(–ù,W)
            min_side = min(low.height, low.width)
            low = TF.center_crop(low, min_side)
            high = TF.center_crop(high, min_side)
        else:
            i, j, h, w = transforms.RandomCrop.get_params(low, (self.crop_size, self.crop_size))
            low = TF.crop(low, i, j, h, w)
            high = TF.crop(high, i, j, h, w)
        return low, high

    def __getitem__(self, idx: int):  # type: ignore[override]
        low_path, high_path = self.pairs[idx]
        img_low = Image.open(low_path).convert("RGB")
        img_high = Image.open(high_path).convert("RGB")

        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó
        img_low, img_high = self.random_crop(img_low, img_high)
        if torch.rand(1) < 0.5:
            img_low = TF.hflip(img_low)
            img_high = TF.hflip(img_high)

        # -> Tensor, [0,1]
        img_low = self.to_tensor(img_low)
        img_high = self.to_tensor(img_high)
        return img_low, img_high

# ---------------------------
# –ú–æ–¥–µ–ª—å Retinexformer (–ø–∞—Ç—á‚Äë–≤–µ—Ä—Å—ñ—è)
# ---------------------------
class Retinexformer(nn.Module):
    def __init__(self, dim: int = 64, depth: int = 4, heads: int = 8, patch_size: int = 8):
        super().__init__()
        self.patch_size = patch_size
        # –ü–∞—Ç—á‚Äë–µ–º–±–µ–¥–∏–Ω–≥ (stride = patch)
        self.enc_embed = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)

        encoder_layer = nn.TransformerEncoderLayer(
            dim, nhead=heads, dim_feedforward=dim * 4, batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)

        decoder_layer = nn.TransformerEncoderLayer(
            dim, nhead=heads, dim_feedforward=dim * 4, batch_first=True
        )
        self.decoder = nn.TransformerEncoder(decoder_layer, num_layers=depth)

        self.dec_proj = nn.Conv2d(dim, 3, kernel_size=3, padding=1)

    def forward(self, x):  # x: (B,3,H,W)
        B, _, H, W = x.shape
        # --- –ï–Ω–∫–æ–¥–µ—Ä ---
        feat = self.enc_embed(x)  # (B, C, H', W')
        H_, W_ = feat.shape[-2:]
        tokens = feat.flatten(2).permute(0, 2, 1)  # (B, N, C) , N = H'√óW'
        tokens = self.encoder(tokens)

        # --- –î–µ–∫–æ–¥–µ—Ä + skip ---
        tokens = self.decoder(tokens) + tokens
        feat_dec = tokens.permute(0, 2, 1).view(B, -1, H_, W_)  # (B, C, H', W')

        # --- Upsample back to full res ---
        feat_up = F.interpolate(
            feat_dec, scale_factor=self.patch_size, mode="bilinear", align_corners=False
        )
        out = torch.sigmoid(self.dec_proj(feat_up))  # [0,1]
        return out

# ---------------------------
# –í—Ç—Ä–∞—Ç–∞
# ---------------------------

def loss_fn(pred, target, l1_weight: float = 0.8):
    l1 = F.l1_loss(pred, target)
    if ssim_fn is not None:
        ssim = 1 - ssim_fn(pred, target)
    else:
        ssim = 0.0
    return l1_weight * l1 + (1 - l1_weight) * ssim

# ---------------------------
# –¶–∏–∫–ª –Ω–∞–≤—á–∞–Ω–Ω—è
# ---------------------------

def train(opt):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[Info] –ü—Ä–∏—Å—Ç—Ä—ñ–π: {device}")

    dataset = LOLPairDataset(opt.dataset_root, crop_size=opt.crop, patch_size=opt.patch)
    dataloader = DataLoader(
        dataset,
        batch_size=opt.batch_size,
        shuffle=True,
        num_workers=4 if os.name != "nt" else 0,
        pin_memory=torch.cuda.is_available(),
    )

    model = Retinexformer(patch_size=opt.patch).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=opt.lr)

    for epoch in range(1, opt.epochs + 1):
        model.train()
        running_loss = 0.0
        for i, (low, high) in enumerate(dataloader, 1):
            low, high = low.to(device, non_blocking=True), high.to(device, non_blocking=True)

            pred = model(low)
            loss = loss_fn(pred, high)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            if i % 10 == 0:
                avg = running_loss / 10
                print(
                    f"Epoch [{epoch}/{opt.epochs}] | Step {i:4d}/{len(dataloader)} | Loss: {avg:.4f}"
                )
                running_loss = 0.0

        # üîí –ß–µ–∫–ø–æ–π–Ω—Ç–∏ (–∑–∞ –ø–æ—Ç—Ä–µ–±–∏)
        # torch.save(model.state_dict(), f"checkpoint_epoch_{epoch}.pth")

# ---------------------------
# CLI –∞—Ä–≥—É–º–µ–Ω—Ç–∏
# ---------------------------

def parse_args():
    p = argparse.ArgumentParser(description="–ù–∞–≤—á–∞–Ω–Ω—è Retinexformer –Ω–∞ LOL")
    p.add_argument("--dataset_root", type=str, default=DEF_DS_ROOT, help="–®–ª—è—Ö –¥–æ –ø–∞–ø–∫–∏ our485")
    p.add_argument("--epochs", type=int, default=DEF_EPOCHS)
    p.add_argument("--batch_size", type=int, default=DEF_BS)
    p.add_argument(
        "--crop", type=int, default=DEF_CROP, help="–†–æ–∑–º—ñ—Ä –≤–∏–ø–∞–¥–∫–æ–≤–æ–≥–æ –∫—Ä–æ–ø–∞ (–∫—Ä–∞—Ç–Ω–∏–π patch)"
    )
    p.add_argument("--patch", type=int, default=DEF_PATCH, help="–†–æ–∑–º—ñ—Ä –ø–∞—Ç—á–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞")
    p.add_argument("--lr", type=float, default=DEF_LR)
    return p.parse_args()


if __name__ == "__main__":
    torch.set_float32_matmul_precision("high")  # –ø—Ä–∏—à–≤–∏–¥—à—É—î –Ω–∞ –Ω–æ–≤–∏—Ö GPU/CPU
    opt = parse_args()
    train(opt)
